{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\tIntroduction\n",
    "The MULTIPLY framework is a new toolkit to extract information from remote sensing data. In contrast to other remote sensing software packages, MULTIPLY is designed to consistently use heterogeneous satellite data (coarse vs high, optical vs microwave). Instead of multiple independent process-chains for each individual satellite, MULTIPLY is a single all-in-coupling framework in order to:\n",
    "*\tCreate consistencies between different land surface products (reducing errors in downstream-processing)\n",
    "*\tUtilize the benefits of multiple satellites (including Sentinel-1, Sentinel-2, MODIS, Landsat), such as \n",
    "    * Higher temporal coverage\n",
    "    * Spectral Sensitivity to more land surface variables\n",
    "*\tQuantify errors of output products, depending on input uncertainties.\n",
    "*\tEnable operational service for different end-users, including: \n",
    "    * Remote sensing consultants\n",
    "    *\tRemote sensing experts\n",
    "In order to achieve these goals MULTIPLY uses a data-assimilation framework developed for the ESA (Gómez-Dans et al. 2016; Lewis et al. 2012). In this framework the state-vector (describing the land surface parameters of interest) are coupled to the observations using radiative transfer models. In addition priori-information (obtained from field-measurements databases) are used to constrain the final results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\tMULTIPLY Concepts\n",
    "## 2.1\tRadiative transfer models\n",
    "In it’s most basic description, a radiative transfer model is a model  capable of (using a specific state-vector) as an input simulateing the different interactions of radiation (light) from source (sun/clouds/..) to sensor (satellite ), using a specific state-vector as an input.  Within the model the most important absorption/transmission and reflection processes by different objects (soil/leaf/atmosphere) is characterized. In this regard there exist many radiative transfer models exist, depending on the level and detail of which important processes are taken into account. In practice, highly complex radiative radiative transfer models usually have a high number of input parameters (to describe in detail the 3D structure of the land surface and the full land surface heterogeneity), while less complex models employ specific hypotheses (such as land surface homogeneity) to require less input parameters. \n",
    "\n",
    "<img src=\"pics/BasicDescriptionRTM2.png\">\n",
    "Figure 1: Basic description of Radiative Transfer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this produces a large variety in radiative transfer models, all of these models have at least one common aspect: they can only be run in ‘forward mode’. This is because completely different scenarios can produce similar remote sensing observations. For example, when a land surface has a very low fractional vegetation cover (~0), different chlorophyll levels will produce similar remote sensing observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2\tData assimilation\n",
    "The most common way to circumvent the shortcoming of radiative transfer models is to ‘guess’ the initial state-vector, run the radiative transfer model using this initial assumption, and compare the output observation against the remote sensing measurement, producing an error. By guessing multiple times,  (for instance by constructing a Look-up-Table (LUT), multiple error-values can be calculated. The lowest error-value than represents our best understanding of the land surface (as long as our initial assumption was close enough to the real land surface description). \n",
    "\n",
    "<img src=\"pics/CreatingLUTs.png\">\n",
    "Figure 2: Creating LUT tables\n",
    "\n",
    "Such an LUT approach has the large disadvantage that it only finds values that were used in the initial guessing; for example if for the Leaf Area Index (LAI) only values [0, 1, 2,.. ,6] were used, this retrieval method will not find the (actual) value of 1.22 m2/m2. \n",
    "The shortcoming of the LUT approach can be greatly to a large part be circumvented if the initial scenarios can be updated on basis of the errors found,  (together with taking into account the sensitivity of the model to the state vector), as illustrated by in Figure 3. Such an iterative approach is in general ‘optimization’ of the radiative transfer model. \n",
    " \n",
    "<img src=\"pics/BasicOptimization.png\">\n",
    "Figure 3: Basic description of Optimization methodology\n",
    "\n",
    "In this approach, it is possible for the state-vector to ‘slowly’ drift away from our prior-value. In addition, the approach is limited to produce only results for when satellite measurements are being performed. In order to solve these shortcomings additional (prior) information can be used within a data-assimilation framework. This is illustrated by Figure 4.\n",
    "\n",
    "<img src=\"pics/DataAssimilation.png\">\n",
    "Figure 4: Data Assimilation of Radiative Transfer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3\tPrior Information\n",
    "In such an data-assimilation approach, an additional error (Error value 2) different from the observational error (Error 1) is defined, describing the deviation of the state-vector from our initial assumption. Here the observational error is defined in units of the observation (e.q. reflectances [-] or radiances [W sr^(-1) m^(-2) nm^(-1)]), while the prior-error is defined in units of the state vector (LAI  [m^2  m^(-2)], Leaf Angle Distribition [deg], …). In order to reconcile these two into one cost function the errors are normalized. For the observational error this is performed by considering the observational errors, while for the prior information this is accomplished by considering the prior uncertainty (for example the standard deviation of long-term measurements of the respective land surface parameter). This leads then to one single cost-function defined as:\n",
    "\n",
    "J_tot=(Error_1)/σ_1 +  (Error_2)/σ_2 =J_obs+J_prior\n",
    "\n",
    "This also immediately provides us with a method on synergistically combining different types of observations, as the observational error can simply be expanded to include multiple observations\n",
    "\n",
    "J_obs=J_obs^1+ J_obs^2+ J_obs^3+⋯\n",
    "\n",
    "In short, each of the error-terms corresponds to different pieces of information added to the system. In this case, even more additional information can also be introduced to the system, be considering the temporal evolution and spatial variation of specific land surface parameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\tThe prototype version overview \n",
    "We hereby provide you with a working prototype  version. This prototype  version is a stripped down version of the actual framework focusing primarily on the optical retrieval using both coarse (MODIS) and high resolution (Sentinel-2) observations. An overview of the full framework is provided in Figure 6. Here the specific modules that were not mature to be incorporated in this release, are the SAR preprocessing, SAR integration, Post processing (for biodiversity and fire disturbance monitoring), as well as the visualization component (highlighted by red circles )\n",
    "\n",
    "<img src=\"pics/MultiplyModules.png\">\n",
    "Figure 5: Overview of MULTIPLY framework. Modules highlighted by a red circle are not included in the prototype version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the version you will be able to download al the required earth observation data (using the data-access component), and afterwards retrieve land surface parameters (using the Inference Engine Module) at  20m resolution using a combination of MODIS coarse resolution data (from the Coarse res Preprocessing module) together with high resolution Sentinel-2 observations (from the high res preprocessing module), together with prior information on the land surface (by the Prior Engine module). In the future it is foreseen that multiple Radiative transfer models can be used, specific to the user requirements, however at the moment the only one available (in the forward operator module) is the emulated version of the PROSAIL radiative transfer mode . This limits the retrieval of land surface parameters to a) leaf-traits (Chlorophyll, Carotonoids, Dry matter, Water Content),  b) Vegetation structural parameters (hotspot parameter, LAI, leaf structure), and c some auxiliary soil parameters (Bsoil/Psoil).\n",
    "\n",
    "<img src=\"pics/MultiplyOutput.png\">\n",
    "Figure 6: Overview of state-vector parameters retrievable on basis of the underlying forward operator (radiative transfer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Usage\n",
    "All the code for the individual modules are is located at https://github.com/multiply-org/. This can be used to setup the MULTIPLY framework on your own computing infrastructure. At present however no deployment setup (in the form of windows-setup-executables, or anaconda package’s) exist. While this is planned further intolater in the project, the focus at this stage is on testing the individual components themselves. Please let us know if you would prefer to install the software yourself on a dedicated computational framework, so that we can investigate how to facilitate this for you. \n",
    "In order to facilitate the testing of the framework itself, we have setup this Virtual Machine on Google Compute Engine, for testing purposes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Please note that for the default setup (currently shown in this python notebook), including downloading the data, atmospherically correcting it, creating user-defined priors and performing the the complete inference (over a single day) for the Jarvjelja Forest site in Estonia, takes ... minutes to complete. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Load internal packages and auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiply_data_access import DataAccessComponent\n",
    "from vm_support import create_sym_links\n",
    "import datetime\n",
    "import glob\n",
    "from typing import List, Optional\n",
    "from vm_support import create_config_file, set_permissions\n",
    "from multiply_prior_engine import PriorEngine\n",
    "\n",
    "tic = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Defining the interfaces to work with the MULTIPLY framework\n",
    "First we need to setup interfaces to work and test with the MULTIPLY framework. In actual operation, these auxiliary interfaces are bundled together within a single python class. However as we would like you to test the different modules individually, we provide direct access to these. For more information on how these interfaces are defined, please check the following link: [MULTIPLY Tools](Tools.py). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vm_support.tools import get_static_data, get_dynamic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vm_support.tools import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vm_support.tools import get_priors, get_priors_from_config_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vm_support.tools import infer, infer3, infer_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vm_support.tools import create_dir, put_data, InvTransformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vm_support.tools import Plot_SRDS,Plot_PRIORS, Plot_TRAITS,Plot_TRAIT_evolution, Plot_Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Running MULTIPLY\n",
    "Below the actual code is provided for running the MULTIPLY framework.\n",
    "We start with setting earth data authentication. This is required to download the MODIS brdf descriptors which are required for the atmospheric correction of the Sentinel-2 data. You can get credentials when you register at https://urs.earthdata.nasa.gov/profile . Registration and use is free of cost. If you do not register, you can only use the MODIS data which has been downloaded in previous runs of the notebook by other users.\n",
    "Also you will need to set up the data stores so that the data access component is working correctly and finds the pre-configured data stores. Both steps only need to be performed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vm_support import set_earth_data_authentication, set_up_data_stores\n",
    "# set_up_data_stores()\n",
    "# username = ''\n",
    "# password = ''\n",
    "# set_earth_data_authentication(username, password) # to download modis data, needs only be done once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Parameters\n",
    "Here you can actually set the parameters for the run. The parameters are as follows:\n",
    "* **roi**: A region of interest, given as a Polygon in WKT format. You can use this tool ( https://arthur-e.github.io/Wicket/sandbox-gmaps3.html ) to easily get definitions of the regions you are interested in in WGS84 coordinates.\n",
    "* **roi_grid**: The EPSG-code of the spatial reference system in which the roi is given. If it is set to 'none', it is assumed that the roi is given in WGS84 coordinates.\n",
    "* **destination_grid**: The EPSG-code of the spatial reference system in which the output shall be given. If it is set to 'none', the platform will attempt to derive it from the roi_grid.\n",
    "* **spatial_resolution**: The resolution the output data is supposed to have, must be a non-negative integer number. The resolution is given in meters and is the same for both dimensions.\n",
    "* **start_time**: The start date of the period you are interested in, must be given in the format 'Year-Month-Day' as below.\n",
    "* **end_time**: The end date of the period you are interested in, must be given in the format 'Year-Month-Day' as below.\n",
    "* **time_step**: The temporal resolution the output is supposed to have. Data will be aggregated over the period denoted by this parameter. Must be a non-negative integer value. The unit is days.\n",
    "* **variables**: The list of the biophysical variables that shall be derived. Please do not change this list, as the underlying forward model requires all of them. The parameters are as follows:\n",
    "  * **n**: Structural parameter\n",
    "  * **cab**: Leaf Chlorophyll Content, given in ug/cm²\n",
    "  * **car**: Leaf Carotonoid Content, given in ug/cm²\n",
    "  * **cb**: Leaf senescent material\n",
    "  * **cw**: Leaf Water Content, given in cm\n",
    "  * **cdm**: Leaf Dry Mass, given in g/cm²\n",
    "  * **lai**: Effective Leaf Area Index, given in m²/m²\n",
    "  * **ala**: Average Leaf Angle, given in degrees\n",
    "  * **bsoil**: Soil Brightness Parameter\n",
    "  * **psoil**: Soil Wetness Parameter\n",
    "* **file_mask**: A file that can be used to explicitly state the region you are interested in. You can also use it to mask out single pixels within this region. If this is not 'none', the aforementioned parameters roi_grid, spatial_resolution, and destination_grid are not used.\n",
    "\n",
    "**HINT**: The platform will perform faster the smaller your roi and the larger the spatial resolution is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Define region of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tartu\n",
    "roi_grid = 'EPSG:4326'\n",
    "roi_centroid = [27.3, 58.3]\n",
    "roi =  'POLYGON((27.1615456795 58.2252523466, ' + \\\n",
    "                '27.1720041127 58.3418423196, ' + \\\n",
    "                '27.3885799673 58.3362682904, ' + \\\n",
    "                '27.3774098023 58.2196966529, ' + \\\n",
    "                '27.1615456795 58.2252523466))'\n",
    "\n",
    "# montesinho\n",
    "# roi_grid = 'EPSG:4326'\n",
    "# roi_centroid = [-6.749181264648428, 41.84622613151897]\n",
    "# roi =  'POLYGON((-6.93 42.00, ' + \\\n",
    "#                 '-6.48 42.00, ' + \\\n",
    "#                 '-6.48 41.68, ' + \\\n",
    "#                 '-6.93 41.68, ' + \\\n",
    "#                 '-6.93 42.00))'\n",
    "\n",
    "# roi_centroid = [-3.50, 37.60]\n",
    "# roi  = 'POLYGON((2.20 37.98, ' + \\\n",
    "#                 '2.40 37.98, ' + \\\n",
    "#                 '2.40 37.35, ' + \\\n",
    "#                 '2.20 37.35, ' + \\\n",
    "#                 '2.20 37.98))'\n",
    "\n",
    "# roi_centroid = [353.20, 41.84]\n",
    "# roi =  'POLYGON((353.07 42.00, ' + \\\n",
    "#                 '353.52 42.00, ' + \\\n",
    "#                 '353.52 41.68, ' + \\\n",
    "#                 '353.07 41.68, ' + \\\n",
    "#                 '353.07 42.00))'\n",
    "\n",
    "\n",
    "destination_grid = 'EPSG:3301'                \n",
    "spatial_resolution = 20 # in m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display\n",
    "\n",
    "google_maps_url = \"http://maps.google.com/maps?q=48.184543+11.213&ie=UTF8&t=h&z=18&output=embed&z=17\"\n",
    "google_maps_url = 'http://maps.google.com/maps?q=%7.5f' % roi_centroid[1] + '+%7.5f' % roi_centroid[0] +'&ie=UTF8&t=h&z=18&output=embed&z=10'\n",
    "IFrame(google_maps_url,800,600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1.2 Define temporal frequency and period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time_as_string = '2018-05-8'\n",
    "# stop_time_as_string = '2018-05-9'\n",
    "\n",
    "# tartu\n",
    "start_time_as_string = '2018-07-24'\n",
    "stop_time_as_string = '2018-07-25'\n",
    "\n",
    "# montesinho\n",
    "# start_time_as_string = '2019-06-29'\n",
    "# stop_time_as_string = '2019-07-30'\n",
    "\n",
    "\n",
    "\n",
    "# start_time_as_string = '2018-05-13'\n",
    "# stop_time_as_string = '2018-05-14'\n",
    "\n",
    "time_step = 10 # in days\n",
    "\n",
    "start_time_as_datetime = datetime.datetime.strptime(start_time_as_string, '%Y-%m-%d')\n",
    "stop_time_as_datetime = datetime.datetime.strptime(stop_time_as_string, '%Y-%m-%d')\n",
    "\n",
    "time_step_as_time_delta = datetime.timedelta(days=time_step)\n",
    "variables = {'n', 'cab', 'car', 'cb', 'cw', 'cdm', 'lai', 'ala', 'bsoil', 'psoil'}\n",
    "\n",
    "# file_mask = \"mask.tif\"\n",
    "file_mask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Setting up the working directory\n",
    "For this notebook, you will operate in your own working directory. All data you use will be copied here, all output will be written here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vm_support import get_working_dir\n",
    "name = 'm1_again51EE'\n",
    "# use previous (non-empty) working directory\n",
    "working_dir = '/Data/test_user_16/' + name\n",
    "\n",
    "#clear working directory\n",
    "working_dir = get_working_dir(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Working directory is {}'.format(working_dir))\n",
    "\n",
    "priors_directory = '{}/priors'.format(working_dir)\n",
    "hres_state_dir = '{}/hresstate'.format(working_dir)\n",
    "modis_directory = '{}/modis'.format(working_dir)\n",
    "state_directory = '{}/state'.format(working_dir)\n",
    "cams_directory = '{}/cams'.format(working_dir)\n",
    "s2_l1c_directory = '{}/s2'.format(working_dir)\n",
    "sdrs_directory = '{}/sdrs'.format(working_dir)\n",
    "biophys_output = '{}/biophys'.format(working_dir)\n",
    "emulators_directory = '{}/emulators'.format(working_dir)\n",
    "dem_directory = '{}/dem'.format(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Acquire Static Data\n",
    "We differentiate between two types of data here: Dynamic and static, meaning: Data which are valid for a certain period of time and data which is valid permanently. The latter is the elevation data of the Digital Elevation Model and the Emulators required for the Atmospheric Correction. We put them in their designated folders before we start our loop through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_access_component = DataAccessComponent()\n",
    "get_static_data(data_access_component=data_access_component, roi=roi,\n",
    "                start_time=start_time_as_string, stop_time=stop_time_as_string, \n",
    "          emulation_directory=emulators_directory, dem_directory=dem_directory, roi_grid=roi_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Infer variables\n",
    "Now we can actually infer the bio-physical variables. For this, we will step though the time grid that we have set up using the start_time, end_time and time_step parameters above.\n",
    "For this stepping, we create a *cursor* that points to the beginning of the time period that we are currently deriving data for.\n",
    "We also define two variables *previous_inference_state* and *updated_inference_state* which will save the state of the inference engine and will ensure that the inference engine can consider the results from its last run.\n",
    "The variable *preprocess_only_region_of_interest* allows to preprocess only the portion of the S2 image that we will consider during the inference. If we process the whole image though, we can save it and use it in later runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thes stepping of time works as follows: Dedicated directories are set up for the MODIS, CAMS, and S2 data. These data are retrieved and put into these directories. After that, pre-processing will take place either on the whole S2 image or on the region of interest. If it has been performed on the whole region, the result will be permanently saved. Next, priors will be derived for every variable and every day within the current period. Having gathered all these, the inference can finally begin. The state of the inference engine is saved and considered during the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Single Run Test\n",
    "First for explanation, we will perform the inference over a single timestep. During this, we will show all the results to indicate the flow of the processings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = start_time_as_datetime\n",
    "previous_inference_state = None #'none'\n",
    "updated_inference_state = 'none'\n",
    "one_day_step = datetime.timedelta(days=1)\n",
    "preprocess_only_region_of_interest = True\n",
    "\n",
    "date_as_string = datetime.datetime.strftime(cursor, '%Y-%m-%d')\n",
    "print('Doing time step starting on {}'.format(date_as_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor += time_step_as_time_delta\n",
    "cursor -= one_day_step\n",
    "if cursor > stop_time_as_datetime:\n",
    "    cursor = stop_time_as_datetime\n",
    "next_date_as_string = datetime.datetime.strftime(cursor, '%Y-%m-%d')\n",
    "cursor += one_day_step\n",
    "cursor_as_string = datetime.datetime.strftime(cursor, '%Y-%m-%d')\n",
    "\n",
    "modis_directory_for_date = '{}/{}'.format(modis_directory, date_as_string)\n",
    "cams_directory_for_date = '{}/{}'.format(cams_directory, date_as_string)\n",
    "s2_l1c_directory_for_date = '{}/{}'.format(s2_l1c_directory, date_as_string)\n",
    "sdrs_directory_for_date = '{}/{}'.format(sdrs_directory, date_as_string)\n",
    "priors_directory_for_date = '{}/{}/'.format(priors_directory, date_as_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = start_time_as_datetime\n",
    "date_as_string = datetime.datetime.strftime(cursor, '%Y-%m-%d')\n",
    "priors_directory_for_date = '{}/{}/'.format(priors_directory, date_as_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dynamic_data(data_access_component, roi, roi_grid, date_as_string, next_date_as_string,\n",
    "                 modis_directory_for_date, cams_directory_for_date, s2_l1c_directory_for_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3  Define Priors\n",
    "You can choose to have the system create it's own 'default' configuration file (using the parameter values, specified earlier). Alternatively, one can make user-specific requests. For example, One can alter the Priors that are going to be used in the inference. If you would like to use these more complex metod, in the actual inference you should provide a link to this user configuration file (or uncomment this line, if one prefers the default configuration). \n",
    "\n",
    "Please be aware that new files have to be created in case of user-defined priors, which can take some time (depending on number of 'user-defined' priors, the size of the area and the duration of the time-period). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the default configuration (of flat priors)\n",
    "config_file = None\n",
    "user_priors = {}\n",
    "user_priors['lai'] = {'mu': 3.0, 'unc': 0.2}\n",
    "# user_priors['cab'] = {'mu': 70.0, 'unc': 70.*0.0001}\n",
    "# user_priors['car'] = {'mu': 10.0, 'unc': 10.0*0.01}\n",
    "# user_priors['cw'] = {'mu': 0.005, 'unc': 0.005*0.01}\n",
    "# user_priors['cdm'] = {'mu': 0.0035, 'unc': 0.0035*0.01}\n",
    "# user_priors['cb'] = {'mu': 0.01, 'unc': 0.01*0.01}\n",
    "# user_priors['n'] = {'mu': 1.6, 'unc': 1.6*0.01}\n",
    "# user_priors['ala'] = {'mu': 70., 'unc': 70*0.01}\n",
    "\n",
    "# user_priors['psoil'] = {'mu': 70., 'unc': 70*0.01}\n",
    "\n",
    "# config_file = None\n",
    "# user_priors = {}\n",
    "# user_priors['lai'] = {'mu': 3.00, 'unc': 0.02}\n",
    "# user_priors['cab'] = {'mu': 70.0, 'unc': 70.*0.0001}\n",
    "# user_priors['car'] = {'mu': 10.0, 'unc': 10.0*0.01}\n",
    "# user_priors['cw'] = {'mu': 0.005, 'unc': 0.005*0.01}\n",
    "# user_priors['cdm'] = {'mu': 0.0035, 'unc': 0.0035*0.01}\n",
    "# user_priors['cb'] = {'mu': 0.01, 'unc': 0.01*0.01}\n",
    "# user_priors['n'] = {'mu': -1.6, 'unc': 1.6*0.01}\n",
    "# user_priors['ala'] = {'mu': 70., 'unc': 70*0.01}\n",
    "\n",
    "# user_priors = {}\n",
    "# user_priors['lai'] = {'mu': -0.2, 'unc': 0.02}\n",
    "# user_priors['cab'] = {'mu': -0.6, 'unc': 0.6*0.01}\n",
    "# user_priors['car'] = {'mu': -0.9, 'unc': 0.9*0.01}\n",
    "# user_priors['cw']  = {'mu': -0.6, 'unc': 0.6*0.01}\n",
    "# user_priors['cdm'] = {'mu': -0.7, 'unc': 0.7*0.01}\n",
    "# user_priors['cb']  = {'mu': -0.2, 'unc': 0.2*0.01}\n",
    "# user_priors['n']   = {'mu': -1.6, 'unc': 1.6*0.01}\n",
    "# user_priors['ala'] = {'mu': -0.9, 'unc': 0.9*0.01}\n",
    "\n",
    "\n",
    "# link to user-configuration file. Comment out to create  default configuration file\n",
    "# config_file = '/home/test_user_16/config_user.yaml'\n",
    "# Please be aware that the working directory in the configuration file should be the same as the working directory \n",
    "# specified above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_file is not None:\n",
    "#     print('choice 1')\n",
    "    get_priors_from_config_file(date_as_string, next_date_as_string, priors_directory_for_date, variables, config_file)\n",
    "else:\n",
    "#     print('choice 2')\n",
    "    get_priors(working_dir, roi, date_as_string, next_date_as_string, time_step, priors_directory_for_date, variables, user_priors)\n",
    "    config_file = working_dir + '/config.yaml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize Priors\n",
    "Plot_PRIORS(roi_centroid,priors_directory_for_date,variables,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u=0\n",
    "# # TEST!!!\n",
    "# user_priors2 = {}\n",
    "# for t in range(Nt):\n",
    "#     varname = list(variables)[t]\n",
    "#     user_priors2[varname] = {'mu':0,'unc':0}#         a = \n",
    "#     user_priors2[varname]['mu'] = -float(classes_u[t,u])      # to indicate priors are already transformed\n",
    "#     user_priors2[varname]['unc'] =  float(classes_u_unc[t,u]) # float(0.000001) #classes_u_unc[t,u]\n",
    "\n",
    "# # modify prior-engine to take 'transformed values directly!!'\n",
    "# get_priors(working_dir, roi, date_as_string, next_date_as_string, time_step, priors_directory_for_date, variables, user_priors2)\n",
    "\n",
    "# # visualize Priors\n",
    "# Plot_PRIORS(roi_centroid,priors_directory_for_date,variables,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 Test Retrieval without Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_new(config_file, date_as_string, cursor_as_string, None, priors_directory_for_date, sdrs_directory_for_date, \n",
    "          updated_inference_state, biophys_output,variables, None, spatial_resolution, roi_grid,destination_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gdal \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "print(biophys_output)\n",
    "variables_subset = variables\n",
    "date = '205'\n",
    "Plot_TRAITS(biophys_output,date,variables_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.4.1 Segment Priors into different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vm_support.tools import Classify_PRIORS3\n",
    "Iclass, classes_u, classes_u_unc = Classify_PRIORS3(biophys_output,variables,date,0.1)\n",
    "\n",
    "Iclass[Iclass<0] = np.nan\n",
    "Nt,Nu = np.shape(classes_u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Tools import Read_TraitPriors2\n",
    "# Data_unc = Read_TraitPriors2(biophys_output,variables, date,'_unc')\n",
    "# Data_unc = np.array(Data_unc)\n",
    "# print(list(variables)[0])\n",
    "# plt.figure(figsize=[20,10])\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.imshow(Data_unc[0,:,:])\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.imshow(Iclass==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5,5])\n",
    "plt.imshow(Iclass)\n",
    "plt.colorbar()\n",
    "plt.title('total number of unique Priors = %d' % np.shape(classes_u)[1])\n",
    "\n",
    "plt.figure(figsize=[20,5])\n",
    "plt.subplot(1,3,1)\n",
    "for t,varname in enumerate(variables):\n",
    "    plt.plot(InvTransformation(varname,classes_u[t,:]))\n",
    "plt.title('Prior untransformed values of classes')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "for t in range(Nt):\n",
    "    plt.plot(classes_u[t,:])\n",
    "plt.title('Prior transformed values of classes')\n",
    "\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "for t in range(Nt):\n",
    "    plt.plot(classes_u_unc[t,:])\n",
    "plt.legend(variables)\n",
    "plt.title('Prior uncertainties of classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct for large variation in uncertainties\n",
    "for t in range(Nt):\n",
    "    # print(np.abs(classes_u_unc[t,]) > (np.mean()+ np.std(classes_u_unc[t,])))\n",
    "    v = classes_u_unc[t,]\n",
    "    p = np.percentile(v,[34, 50, 68])\n",
    "    v[(v<p[0]) + (v>p[2])] = p[1]\n",
    "    classes_u_unc[t,] = v\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5 Preprocessing Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vm_support.tools import Check_SDRS_Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dynamic_data(data_access_component, roi, roi_grid, date_as_string, next_date_as_string,\n",
    "#                  modis_directory_for_date, cams_directory_for_date, s2_l1c_directory_for_date)\n",
    "preprocess(s2_l1c_directory_for_date, modis_directory_for_date, emulators_directory, cams_directory_for_date, \n",
    "           dem_directory, sdrs_directory_for_date, roi)\n",
    "Check_SDRS_Files(sdrs_directory_for_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the atmospheric correction\n",
    "Productnr = 0\n",
    "Plot_SRDS(sdrs_directory, Productnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Productnr = 1\n",
    "#Plot_SRDS(sdrs_directory, Productnr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Check_SDRS_Files(sdrs_directory_for_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.6  Retrieval with Observations\n",
    "Due to an instability in the inferenc engine (that is currently being investigated). The inference only works with flat priors. Therefore we first run (above) the inference with purely priors. This will results in biophysical parameters for the selected area to be equivelent to the prior values. Afterwards we segment the retrieval to be run for individual prior-classes. Basically then we \n",
    "1) require priors to be segmented into different classes \n",
    "2) for each class,  use the 'user' option to define flat priors with the appropriate mean/unc values\n",
    "3) run the inference\n",
    "4) merge everything together. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.6.1 Run the inference for the different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nt, Nu = np.shape(classes_u)\n",
    "# classes_u = float(classes_u)\n",
    "# classes_u_unc = float(classes_u_unc)\n",
    "\n",
    "# indicate that the priors are already 'transformed' \n",
    "# classes_u_t =  - classes_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx, Ny = np.shape(Iclass)\n",
    "Nx, Ny, = np.shape(Iclass)\n",
    "Traits = np.zeros((Nx,Ny,Nt,Nu))\n",
    "Traits_unc = np.zeros((Nx,Ny,Nt,Nu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vm_support.tools import Read_TraitPriors2\n",
    "\n",
    "for u in range(Nu):\n",
    "    # define flat_priors\n",
    "    user_priors2 = {}\n",
    "    for t in range(Nt):\n",
    "        varname = list(variables)[t]\n",
    "        user_priors2[varname] = {'mu':0,'unc':0}#         a = \n",
    "        user_priors2[varname]['mu'] = -float(classes_u[t,u])      # to indicate priors are already transformed\n",
    "        user_priors2[varname]['unc'] =  float(classes_u_unc[t,u]) # float(0.000001) #classes_u_unc[t,u]\n",
    "        \n",
    "    # modify prior-engine to take 'transformed values directly!!'\n",
    "    get_priors(working_dir, roi, date_as_string, next_date_as_string, time_step, priors_directory_for_date, variables, user_priors2)\n",
    "\n",
    "    # run inference\n",
    "    infer_new(config_file, date_as_string, cursor_as_string, None, priors_directory_for_date, sdrs_directory_for_date, \n",
    "          updated_inference_state, biophys_output,variables, None, spatial_resolution, roi_grid,destination_grid)\n",
    "\n",
    "    # read retrieved values + uncertainty\n",
    "    Data = Read_TraitPriors2(biophys_output,variables, date, str='')\n",
    "    Data_unc = Read_TraitPriors2(biophys_output,variables, date, str='_unc')\n",
    "    Data = np.array(Data)\n",
    "    Data_unc = np.array(Data_unc)\n",
    "    Traits[:,:,:,u] = np.transpose(Data,(1,2,0))\n",
    "    Traits_unc[:,:,:,u] = np.transpose(Data_unc,(1,2,0))\n",
    "Traits2 = Traits*1.\n",
    "Traits_unc2 = Traits_unc*1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize construction of final results from prior-classes\n",
    "t=0 \n",
    "varname = list(variables)[t]\n",
    "\n",
    "plt.figure(figsize=[20,15])\n",
    "for u in range(Nu):\n",
    "    V = Traits[:,:,t,u]*(Iclass==u)\n",
    "    V[V==0]=np.nan\n",
    "    V = InvTransformation(varname,V)\n",
    "    \n",
    "    plt.subplot(2,4,u+1)\n",
    "    plt.imshow(V)\n",
    "    plt.colorbar()\n",
    "    plt.title('Class = {}'.format(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge traits into 1 map\n",
    "Trait = np.zeros((Nx,Ny,Nt))\n",
    "Trait_unc = np.zeros((Nx,Ny,Nt))\n",
    "for t,varname in enumerate(variables):\n",
    "    for u in range(Nu):\n",
    "        Trait[:,:,t]     = Trait[:,:,t]     + (Iclass==u)*Traits[:,:,t,u]\n",
    "        Trait_unc[:,:,t] = Trait_unc[:,:,t] +  (Iclass==u)*Traits_unc[:,:,t,u]\n",
    "    \n",
    "# visualize\n",
    "t=0\n",
    "varname = list(variables)[0]\n",
    "plt.figure(figsize=[20,5])\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(Trait[:,:,t])\n",
    "plt.colorbar()\n",
    "plt.title('Retrieved {}'.format(varname))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(Trait_unc[:,:,t])\n",
    "plt.colorbar()\n",
    "plt.title('Uncertainties {}'.format(varname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.6.2 Store merged traits values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t,varname in enumerate(variables):    \n",
    "    data_file = glob.glob(biophys_output+ '/' + varname + '*'+ date +  '' + '.tif')[0]\n",
    "    data_set = gdal.Open(data_file, gdal.GA_Update)\n",
    "    data_set.GetRasterBand(1).WriteArray(Trait[:,:,t])\n",
    "    data_set = None\n",
    "\n",
    "    data_file_unc = glob.glob(biophys_output+ '/' + varname + '*'+ date +  '_unc' + '.tif')[0]\n",
    "    data_set = gdal.Open(data_file, gdal.GA_Update)\n",
    "    data_set.GetRasterBand(1).WriteArray(Trait_unc[:,:,t])\n",
    "    data_set = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.6.3  Visualize retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gdal \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "print(biophys_output)\n",
    "variables_subset = variables\n",
    "date = '205'\n",
    "\n",
    "Plot_TRAITS(biophys_output,date,variables_subset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = datetime.datetime.now()\n",
    "T = (toc - tic).seconds\n",
    "days = np.floor(T/60./60./24)\n",
    "res = T - days*60.*60.*24.\n",
    "hours = np.floor(res/60./60.)\n",
    "res = T - hours\n",
    "minutes = np.ceil(res/60.)\n",
    "\n",
    "print('The run took %02d:%02d:%02d minutes ' % (days,hours,minutes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Read Data \n",
    "All the variables are stored within the user directory. However for visualisation purposes we here only provide you to investigate a 'single variable of interest. Please find all the output files located at the following directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(biophys_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "variable_of_interest = 'lai'\n",
    "\n",
    "data_files = set(glob.glob(biophys_output + '/*' + variable_of_interest + '*')) - \\\n",
    "            set(glob.glob(biophys_output + '/*unc*'))\n",
    "num_of_data_sets = len(data_files)\n",
    "\n",
    "Data = []\n",
    "print('reading %s files into memory' % variable_of_interest)\n",
    "for data_file in data_files:\n",
    "    data_set = gdal.Open(data_file)\n",
    "    data = data_set.ReadAsArray(0)\n",
    "    data[np.where(data<1e-8)]=np.NaN\n",
    "    data[np.where(data>0.99)]=np.NaN\n",
    "    \n",
    "    Data.append(data)    \n",
    "print('Finished reading all %s files' % variable_of_interest)    \n",
    "\n",
    "data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Process Data\n",
    "Within MULTIPLY we use transformation on some of the variables in order to (approximately) linearize the  radiative transfer model. In this sense, LAI within the state-vector is corrected for the exponential extinction of radiation that occurs witin the canopy. \n",
    "\n",
    "LAI = exp(LAI/-2.0)\n",
    "\n",
    "This linearisation speeds-up the inference and also allows us to propagate the uncertainties better. However it comes at the expense that the output results of the inference are not in real-units (for LAI -> [m2/m2]). Instead they are in transformed space, ranging from 0.0 (for dense vegetation) - 1.0 (for bare soil). This is highlighted below. \n",
    "\n",
    "Please also note that for LAI we use  relatively coarse resolution prior information. Therefore for retrievals over short time periods (particular locations with high cloud cover), this coarse resolution will be apparant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAI is not the only variable within the state-vector that are defined in transformed equivalents. The other transformed parameters are: leaf chlorophyll content (cab),leaf  water content (cw), leaf dry matter (cdm), and leaf average angle.\n",
    "\n",
    "\\begin{equation*}\n",
    "    cab_{real}    = (-100.0)* ln( cab_{t} )\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    cw_{real}    = (-1/50.0)* log( data )\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}    \n",
    "    cdm_{real}    = (-1/100.0)* log( cdm_{t} )\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}    \n",
    "    LAI_{real}    = -2.0* log( LAI_{t} )\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}    \n",
    "    ALA_{real}    = 90.0 * ALA_{t}\n",
    "\\end{equation*}\n",
    "\n",
    "A Inverse Transformation module is provided within Tools.py to facilitate the retrieval of the actual values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_t = InvTransformation(variable_of_interest,Data)\n",
    "print('Transformed (min,mean,max) Values  are  [%3.2e, %3.2e, %3.2e]' % (np.nanmin(Data), np.nanmean(Data), np.nanmax(Data)))\n",
    "print('Real (min,mean,max) Values are  [%3.2e, %3.2e, %3.2e]' % (np.nanmin(Data_t), np.nanmean(Data_t), np.nanmax(Data_t)))\n",
    "      \n",
    "Plot_Transformation(Data,Data_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Plot Temporal evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_TRAIT_evolution(Data_t,variable_of_interest)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Download the Data\n",
    "We first need to compile all the biophysical parameters into a single zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(f'tar -cvf {working_dir}/biophys_download.zip {biophys_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "FileLinks('download')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = datetime.datetime.now()\n",
    "T = (toc - tic).seconds\n",
    "days = np.floor(T/60./60./24)\n",
    "res = T - days*60.*60.*24.\n",
    "hours = np.floor(res/60./60.)\n",
    "res = T - hours\n",
    "minutes = np.ceil(res/60.)\n",
    "\n",
    "print('The run took %02d:%02d:%02d minutes ' % (days,hours,minutes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the Contributing team\n",
    "<img src=\"pics/Logos.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
